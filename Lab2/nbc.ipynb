{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manisha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from typing import List, Dict, Tuple\n",
    "nltk.download('punkt')\n",
    "#\"HP1\", \"HP2\", \"HP3\", \"HP4\",\n",
    "y_book_array = [\"HP1\", \"HP2\", \"HP3\", \"HP4\", \"HP5\", \"HP6\", \"HP7\"]\n",
    "\n",
    "\n",
    "# Function to read and preprocess text\n",
    "def read_and_preprocess(files):\n",
    "    corpus = []\n",
    "    book_tokens = []\n",
    "   \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read().lower()\n",
    "            # Split text into pages\n",
    "            pages = text.split('\\n')\n",
    "            \n",
    "            book_pages_tokens = []\n",
    "            for page in pages:\n",
    "                # Remove punctuation\n",
    "                page = re.sub(r'[^\\w\\s]', '', page)\n",
    "                # Tokenize\n",
    "                tokens = word_tokenize(page)\n",
    "                corpus.extend(tokens)\n",
    "                book_pages_tokens.append(tokens)\n",
    "            \n",
    "            book_tokens.append(book_pages_tokens)\n",
    "    return corpus, book_tokens\n",
    "\n",
    "# Specify the files for the seven books\n",
    "files = ['HarryPotter/HP1.txt', 'HarryPotter/HP2.txt', 'HarryPotter/HP3.txt', 'HarryPotter/HP4.txt','HarryPotter/HP5.txt', 'HarryPotter/HP6.txt', 'HarryPotter/HP7.txt']\n",
    "#files = ['HarryPotter/HP4.txt','HarryPotter/HP5.txt', 'HarryPotter/HP6.txt', 'HarryPotter/HP7.txt']\n",
    "# Preprocess the text\n",
    "# tokens is now a 3d array with each sub array has sub-subarrays that are each page for each book\n",
    "corpus, tokens = read_and_preprocess(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigrams</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mr, and, mrs, dursley, of, number, four, priv...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[met, for, several, years, in, fact, mrs, durs...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, cat, it, stared, back, as, mr, dursley, ...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[calls, and, shouted, a, bit, more, he, was, i...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[he, found, it, a, lot, harder, to, concentrat...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>[youre, right, sorry, said, ron, but, unable, ...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>[no, said, harry, firmly, you, and, a1, will, ...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>[ginny, kissed, albus, goodbye, see, you, at, ...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>[he, had, never, told, any, of, his, children,...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>[]</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4707 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               trigrams book\n",
       "0     [mr, and, mrs, dursley, of, number, four, priv...  HP1\n",
       "1     [met, for, several, years, in, fact, mrs, durs...  HP1\n",
       "2     [the, cat, it, stared, back, as, mr, dursley, ...  HP1\n",
       "3     [calls, and, shouted, a, bit, more, he, was, i...  HP1\n",
       "4     [he, found, it, a, lot, harder, to, concentrat...  HP1\n",
       "...                                                 ...  ...\n",
       "4702  [youre, right, sorry, said, ron, but, unable, ...  HP7\n",
       "4703  [no, said, harry, firmly, you, and, a1, will, ...  HP7\n",
       "4704  [ginny, kissed, albus, goodbye, see, you, at, ...  HP7\n",
       "4705  [he, had, never, told, any, of, his, children,...  HP7\n",
       "4706                                                 []  HP7\n",
       "\n",
       "[4707 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_ngrams_from_tokens(tokens, book,  n):\n",
    "    # Step 1: Initialize an empty list to store N-grams\n",
    "    n_grams = []\n",
    "    y_array = []\n",
    "    # Step 2: Generate N-grams\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        n_gram = ' '.join(tokens[i:i+n])\n",
    "        n_grams.append(n_gram)\n",
    "        y_array.append(book)\n",
    "    return n_grams, y_array\n",
    "\n",
    "for i in range(len(files)):\n",
    "    if i ==0:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        # for each page \n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 1)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        x_y_trigrams = pd.DataFrame({'trigrams': pages_trigramed, 'book': pages_y_array})\n",
    "    else:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 1)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        book_x_y_trigrams = pd.DataFrame({\"trigrams\": pages_trigramed, \"book\": pages_y_array})\n",
    "        x_y_trigrams = pd.concat([x_y_trigrams, book_x_y_trigrams], ignore_index=True)\n",
    "    \n",
    "display(x_y_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(x_y_trigrams[\"trigrams\"], x_y_trigrams[\"book\"], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HP1': 1.0, 'HP2': 1.0, 'HP3': 1.0, 'HP4': 1.0, 'HP5': 1.0, 'HP6': 1.0, 'HP7': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(None, classes=classes, y=y_train)\n",
    "    return dict(zip(classes, class_weights))\n",
    "\n",
    "# Example usage\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Calculating the priors\n",
    "def get_priors(y_train):\n",
    "    book_counts = Counter(y_train)\n",
    "    total_count = len(y_train)\n",
    "    priors = {}\n",
    "    for i in book_counts:\n",
    "        priors.update({i: book_counts[i]/total_count})\n",
    "    return priors\n",
    "\n",
    "#calculating the occurences\n",
    "\n",
    "def get_occurrences(n_gram: str, x_train: List[List[str]], y_train: List[str], classes: List[str]) -> Dict[str, int]:\n",
    "    occurrences = {c: 0 for c in classes}\n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        if n_gram in trigrams:\n",
    "            occurrences[book] += 1\n",
    "    return occurrences\n",
    "    \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def get_likelihood(book: str, n_gram: str, n_gram_map: Dict[str, Dict[str, int]], total_trigrams_per_class: Dict[str, int], vocab_size: int) -> float:\n",
    "    delta = 0.1 # Laplace smoothing\n",
    "    \n",
    "    numerator = delta + n_gram_map.get(n_gram, {}).get(book, 0)\n",
    "    denominator = delta * vocab_size + total_trigrams_per_class[book]\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def n_gram_mapping(x_train: List[List[str]], y_train: List[str], classes: List[str]) -> Tuple[Dict[str, Dict[str, int]], Dict[str, int]]:\n",
    "    n_gram_map = defaultdict(lambda: defaultdict(int))\n",
    "    total_trigrams_per_class = {c: 0 for c in classes}\n",
    "    \n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        for n_gram in trigrams:\n",
    "            n_gram_map[n_gram][book] += 1\n",
    "            total_trigrams_per_class[book] += 1\n",
    "    \n",
    "    return n_gram_map, total_trigrams_per_class\n",
    "\n",
    "\n",
    "def prob_page_in_book(page, priors, n_gram_map, total_trigrams_per_class, classes, vocab_size, class_weights):\n",
    "    log_probs = []\n",
    "    \n",
    "    for c in classes:\n",
    "        #log_prob = np.log(priors[c])\n",
    "        log_prob = np.log(priors[c]* class_weights[c])\n",
    "        for n_gram in page:\n",
    "            likelihood = get_likelihood(c, n_gram, n_gram_map, total_trigrams_per_class, vocab_size)\n",
    "            log_prob += np.log(likelihood)\n",
    "            \n",
    "        log_probs.append(log_prob)\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "def predict(X: List[List[str]], y: List[str], X_test: List[List[str]], classes: List[str]) -> List[str]:\n",
    "    priors = get_priors(y)\n",
    "    n_gram_map, total_trigrams_per_class = n_gram_mapping(X, y, classes)\n",
    "    vocab_size = len(set(n_gram for page in X for n_gram in page))\n",
    "    \n",
    "    class_weights = calculate_class_weights(y)\n",
    "    \n",
    "    predictions = []\n",
    "    for page in X_test:\n",
    "        log_probs = prob_page_in_book(page, priors, n_gram_map, total_trigrams_per_class, classes, vocab_size, class_weights)\n",
    "        predictions.append(classes[np.argmax(log_probs)])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.73%\n",
      "Accuracy: 80.25%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = predict(X_train, y_train, X_val, y_book_array)\n",
    "accuracy = sum(p == a for p, a in zip(predictions,  y_val)) / len(y_val)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict(X_train, y_train, X_test, y_book_array)\n",
    "accuracy = sum(p == a for p, a in zip(predictions, y_test)) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manisha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from typing import List, Dict, Tuple\n",
    "nltk.download('punkt')\n",
    "#\"HP1\", \"HP2\", \"HP3\", \"HP4\",\n",
    "y_book_array = [\"HP1\", \"HP2\", \"HP3\", \"HP4\", \"HP5\", \"HP6\", \"HP7\"]\n",
    "\n",
    "\n",
    "# Function to read and preprocess text\n",
    "def read_and_preprocess(files):\n",
    "    corpus = []\n",
    "    book_tokens = []\n",
    "   \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read().lower()\n",
    "            # Split text into pages\n",
    "            pages = text.split('\\n')\n",
    "            \n",
    "            book_pages_tokens = []\n",
    "            for page in pages:\n",
    "                # Remove punctuation\n",
    "                page = re.sub(r'[^\\w\\s]', '', page)\n",
    "                # Tokenize\n",
    "                tokens = word_tokenize(page)\n",
    "                corpus.extend(tokens)\n",
    "                book_pages_tokens.append(tokens)\n",
    "            \n",
    "            book_tokens.append(book_pages_tokens)\n",
    "    return corpus, book_tokens\n",
    "\n",
    "# Specify the files for the seven books\n",
    "files = ['HarryPotter/HP1.txt', 'HarryPotter/HP2.txt', 'HarryPotter/HP3.txt', 'HarryPotter/HP4.txt','HarryPotter/HP5.txt', 'HarryPotter/HP6.txt', 'HarryPotter/HP7.txt']\n",
    "#files = ['HarryPotter/HP4.txt','HarryPotter/HP5.txt', 'HarryPotter/HP6.txt', 'HarryPotter/HP7.txt']\n",
    "# Preprocess the text\n",
    "# tokens is now a 3d array with each sub array has sub-subarrays that are each page for each book\n",
    "corpus, tokens = read_and_preprocess(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigrams</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mr and mrs, and mrs dursley, mrs dursley of, ...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[met for several, for several years, several y...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the cat it, cat it stared, it stared back, st...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[calls and shouted, and shouted a, shouted a b...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[he found it, found it a, it a lot, a lot hard...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>[youre right sorry, right sorry said, sorry sa...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>[no said harry, said harry firmly, harry firml...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>[ginny kissed albus, kissed albus goodbye, alb...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>[he had never, had never told, never told any,...</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>[]</td>\n",
       "      <td>HP7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4707 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               trigrams book\n",
       "0     [mr and mrs, and mrs dursley, mrs dursley of, ...  HP1\n",
       "1     [met for several, for several years, several y...  HP1\n",
       "2     [the cat it, cat it stared, it stared back, st...  HP1\n",
       "3     [calls and shouted, and shouted a, shouted a b...  HP1\n",
       "4     [he found it, found it a, it a lot, a lot hard...  HP1\n",
       "...                                                 ...  ...\n",
       "4702  [youre right sorry, right sorry said, sorry sa...  HP7\n",
       "4703  [no said harry, said harry firmly, harry firml...  HP7\n",
       "4704  [ginny kissed albus, kissed albus goodbye, alb...  HP7\n",
       "4705  [he had never, had never told, never told any,...  HP7\n",
       "4706                                                 []  HP7\n",
       "\n",
       "[4707 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_ngrams_from_tokens(tokens, book,  n):\n",
    "    # Step 1: Initialize an empty list to store N-grams\n",
    "    n_grams = []\n",
    "    y_array = []\n",
    "    # Step 2: Generate N-grams\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        n_gram = ' '.join(tokens[i:i+n])\n",
    "        n_grams.append(n_gram)\n",
    "        y_array.append(book)\n",
    "    return n_grams, y_array\n",
    "\n",
    "for i in range(len(files)):\n",
    "    if i ==0:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        # for each page \n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 3)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        x_y_trigrams = pd.DataFrame({'trigrams': pages_trigramed, 'book': pages_y_array})\n",
    "    else:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 3)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        book_x_y_trigrams = pd.DataFrame({\"trigrams\": pages_trigramed, \"book\": pages_y_array})\n",
    "        x_y_trigrams = pd.concat([x_y_trigrams, book_x_y_trigrams], ignore_index=True)\n",
    "    \n",
    "display(x_y_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(x_y_trigrams[\"trigrams\"], x_y_trigrams[\"book\"], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HP1': 1.0, 'HP2': 1.0, 'HP3': 1.0, 'HP4': 1.0, 'HP5': 1.0, 'HP6': 1.0, 'HP7': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(None, classes=classes, y=y_train)\n",
    "    return dict(zip(classes, class_weights))\n",
    "\n",
    "# Example usage\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Calculating the priors\n",
    "def get_priors(y_train):\n",
    "    book_counts = Counter(y_train)\n",
    "    total_count = len(y_train)\n",
    "    priors = {}\n",
    "    for i in book_counts:\n",
    "        priors.update({i: book_counts[i]/total_count})\n",
    "    return priors\n",
    "\n",
    "#calculating the occurences\n",
    "\n",
    "def get_occurrences(n_gram: str, x_train: List[List[str]], y_train: List[str], classes: List[str]) -> Dict[str, int]:\n",
    "    occurrences = {c: 0 for c in classes}\n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        if n_gram in trigrams:\n",
    "            occurrences[book] += 1\n",
    "    return occurrences\n",
    "    \n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def get_likelihood(book: str, n_gram: str, n_gram_map: Dict[str, Dict[str, int]], total_trigrams_per_class: Dict[str, int], vocab_size: int) -> float:\n",
    "    delta = 1.0 # Laplace smoothing\n",
    "    \n",
    "    numerator = delta + n_gram_map.get(n_gram, {}).get(book, 0)\n",
    "    denominator = delta * vocab_size + total_trigrams_per_class[book]\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def n_gram_mapping(x_train: List[List[str]], y_train: List[str], classes: List[str]) -> Tuple[Dict[str, Dict[str, int]], Dict[str, int]]:\n",
    "    n_gram_map = defaultdict(lambda: defaultdict(int))\n",
    "    total_trigrams_per_class = {c: 0 for c in classes}\n",
    "    \n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        for n_gram in trigrams:\n",
    "            n_gram_map[n_gram][book] += 1\n",
    "            total_trigrams_per_class[book] += 1\n",
    "    \n",
    "    return n_gram_map, total_trigrams_per_class\n",
    "\n",
    "\n",
    "def prob_page_in_book(page, priors, n_gram_map, total_trigrams_per_class, classes, vocab_size, class_weights):\n",
    "    log_probs = []\n",
    "    \n",
    "    for c in classes:\n",
    "        #log_prob = np.log(priors[c])\n",
    "        log_prob = np.log(priors[c]* class_weights[c])\n",
    "        for n_gram in page:\n",
    "            likelihood = get_likelihood(c, n_gram, n_gram_map, total_trigrams_per_class, vocab_size)\n",
    "            log_prob += np.log(likelihood)\n",
    "            \n",
    "        log_probs.append(log_prob)\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "def predict(X: List[List[str]], y: List[str], X_test: List[List[str]], classes: List[str]) -> List[str]:\n",
    "    priors = get_priors(y)\n",
    "    n_gram_map, total_trigrams_per_class = n_gram_mapping(X, y, classes)\n",
    "    vocab_size = len(set(n_gram for page in X for n_gram in page))\n",
    "    \n",
    "    class_weights = calculate_class_weights(y)\n",
    "    \n",
    "    predictions = []\n",
    "    for page in X_test:\n",
    "        log_probs = prob_page_in_book(page, priors, n_gram_map, total_trigrams_per_class, classes, vocab_size, class_weights)\n",
    "        predictions.append(classes[np.argmax(log_probs)])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Validation: 59.43%\n",
      "Accuracy: 57.75%\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_train, y_train, X_val, y_book_array)\n",
    "accuracy = sum(p == a for p, a in zip(predictions,  y_val)) / len(y_val)\n",
    "print(f\"Accuracy - Validation: {accuracy * 100:.2f}%\")\n",
    "\n",
    "predictions = predict(X_train, y_train, X_test, y_book_array)\n",
    "accuracy = sum(p == a for p, a in zip(predictions, y_test)) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
