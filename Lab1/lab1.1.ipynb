{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manisha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read and preprocess text\n",
    "def read_and_preprocess(files):\n",
    "    corpus = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read().lower()\n",
    "            # Remove punctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            corpus.extend(tokens)\n",
    "    return corpus\n",
    "\n",
    "# Specify the files for the seven books\n",
    "files = ['harry_potter(1)/HP1.txt', 'harry_potter(1)/HP2.txt', 'harry_potter(1)/HP3.txt', 'harry_potter(1)/HP4.txt', 'harry_potter(1)/HP5.txt', 'harry_potter(1)/HP6.txt', 'harry_potter(1)/HP7.txt']\n",
    "\n",
    "# Preprocess the text\n",
    "corpus = read_and_preprocess(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary and mappings\n",
    "unique_words = list(set(corpus))\n",
    "word_to_idx = {word: i for i, word in enumerate(unique_words)}\n",
    "idx_to_word = {i: word for i, word in enumerate(unique_words)}\n",
    "\n",
    "# Label encoding for corpus\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(corpus)\n",
    "\n",
    "# One-hot encoding for integer encoded corpus\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "def generate_training_data(corpus, word_to_idx, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(corpus)):\n",
    "        target_word = word_to_idx[corpus[i]]\n",
    "        context_words = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j == 0 or i + j < 0 or i + j >= len(corpus):\n",
    "                continue\n",
    "            context_word = word_to_idx[corpus[i + j]]\n",
    "            context_words.append(context_word)\n",
    "        X.extend([target_word] * len(context_words))\n",
    "        y.extend(context_words)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 1\n",
    "X, y = generate_training_data(corpus, word_to_idx, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manisha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-08-07 15:19:02.969601: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.811822891235352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:36:41.087451: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 7.89631986618042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 15:54:09.418591: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 7.531877040863037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 16:11:59.641113: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 7.273609161376953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 16:29:35.557423: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 7.095487594604492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 16:46:52.135773: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 6.967774391174316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 17:04:25.242619: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 6.870762348175049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 17:21:52.325748: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 6.7932305335998535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 17:40:28.510307: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 6.7295002937316895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 17:57:53.176626: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 6.676143646240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 18:15:32.044866: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 6.630636692047119\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = len(unique_words)\n",
    "embedding_dim = 100  # Dimension of the embedding vectors\n",
    "\n",
    "# Define the model\n",
    "class Word2VecModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.flatten(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred))\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, X_train, y_train, epochs=50, batch_size=40):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_avg = tf.metrics.Mean()\n",
    "        for X_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(X_batch, training=True)\n",
    "                loss = compute_loss(y_batch, y_pred)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            epoch_loss_avg.update_state(loss)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss_avg.result().numpy()}\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "train_model(model, X_train, y_train)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top related words to 'harry':\n",
      "hermione: 0.6103\n",
      "ron: 0.5970\n",
      "james: 0.5350\n",
      "dirk: 0.4983\n",
      "ringleaders: 0.4949\n",
      "feebly: 0.4848\n",
      "witheringly: 0.4692\n",
      "aberforth: 0.4628\n",
      "magorian: 0.4619\n",
      "mournfully: 0.4601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a model to extract embeddings\n",
    "embedding_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=1).output)\n",
    "embeddings = embedding_model.get_weights()[0]  # Get the weights of the embedding layer\n",
    "\n",
    "# Function to get embedding for a word\n",
    "def get_embedding(word):\n",
    "    word_idx = word_to_idx[word]\n",
    "    return embeddings[word_idx]\n",
    "\n",
    "# Compute similarity between the target word and all other words\n",
    "def get_most_similar_words(target_word, top_n=10):\n",
    "    target_embedding = get_embedding(target_word)\n",
    "    similarities = []\n",
    "    \n",
    "    for idx, word in idx_to_word.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_embedding = embeddings[idx]\n",
    "        similarity = cosine_similarity([target_embedding], [word_embedding])[0][0]\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "target_word = \"harry\"\n",
    "top_related_words = get_most_similar_words(target_word, top_n=10)\n",
    "\n",
    "print(f\"Top related words to '{target_word}':\")\n",
    "for word, similarity in top_related_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top related words to 'harry':\n",
      "hermione: 0.7459\n",
      "ron: 0.7029\n",
      "neville: 0.6917\n",
      "hagrid: 0.6775\n",
      "he: 0.6774\n",
      "snape: 0.6748\n",
      "fred: 0.6644\n",
      "wormtail: 0.6555\n",
      "dudley: 0.6297\n",
      "she: 0.6290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a model to extract embeddings\n",
    "embedding_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=1).output)\n",
    "embeddings = embedding_model.get_weights()[0]  # Get the weights of the embedding layer\n",
    "\n",
    "# Function to get embedding for a word\n",
    "def get_embedding(word):\n",
    "    word_idx = word_to_idx[word]\n",
    "    return embeddings[word_idx]\n",
    "\n",
    "# Compute similarity between the target word and all other words\n",
    "def get_most_similar_words(target_word, top_n=10):\n",
    "    target_embedding = get_embedding(target_word)\n",
    "    similarities = []\n",
    "    \n",
    "    for idx, word in idx_to_word.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_embedding = embeddings[idx]\n",
    "        similarity = cosine_similarity([target_embedding], [word_embedding])[0][0]\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "target_word = \"harry\"\n",
    "top_related_words = get_most_similar_words(target_word, top_n=10)\n",
    "\n",
    "print(f\"Top related words to '{target_word}':\")\n",
    "for word, similarity in top_related_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
