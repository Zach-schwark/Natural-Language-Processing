{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manisha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read and preprocess text\n",
    "def read_and_preprocess(files):\n",
    "    corpus = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read().lower()\n",
    "            # Remove punctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            corpus.extend(tokens)\n",
    "    return corpus\n",
    "\n",
    "# Specify the files for the seven books\n",
    "files = ['harry_potter(1)/HP1.txt', 'harry_potter(1)/HP2.txt', 'harry_potter(1)/HP3.txt', 'harry_potter(1)/HP4.txt', 'harry_potter(1)/HP5.txt', 'harry_potter(1)/HP6.txt', 'harry_potter(1)/HP7.txt']\n",
    "\n",
    "# Preprocess the text\n",
    "corpus = read_and_preprocess(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary and mappings\n",
    "unique_words = list(set(corpus))\n",
    "word_to_idx = {word: i for i, word in enumerate(unique_words)}\n",
    "idx_to_word = {i: word for i, word in enumerate(unique_words)}\n",
    "\n",
    "# Label encoding for corpus\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(corpus)\n",
    "\n",
    "# One-hot encoding for integer encoded corpus\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "def generate_training_data(corpus, word_to_idx, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(corpus)):\n",
    "        target_word = word_to_idx[corpus[i]]\n",
    "        context_words = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j == 0 or i + j < 0 or i + j >= len(corpus):\n",
    "                continue\n",
    "            context_word = word_to_idx[corpus[i + j]]\n",
    "            context_words.append(context_word)\n",
    "        X.extend([target_word] * len(context_words))\n",
    "        y.extend(context_words)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 1\n",
    "X, y = generate_training_data(corpus, word_to_idx, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manisha/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 11ms/step - loss: 6.6995 - val_loss: 6.1416\n",
      "Epoch 2/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 11ms/step - loss: 6.0079 - val_loss: 6.0751\n",
      "Epoch 3/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 13ms/step - loss: 5.8778 - val_loss: 6.0864\n",
      "Epoch 4/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 13ms/step - loss: 5.8123 - val_loss: 6.1211\n",
      "Epoch 5/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 12ms/step - loss: 5.7658 - val_loss: 6.1489\n",
      "Epoch 6/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 12ms/step - loss: 5.7187 - val_loss: 6.1729\n",
      "Epoch 7/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 12ms/step - loss: 5.6814 - val_loss: 6.1889\n",
      "Epoch 8/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 13ms/step - loss: 5.6504 - val_loss: 6.1947\n",
      "Epoch 9/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 13ms/step - loss: 5.6350 - val_loss: 6.2012\n",
      "Epoch 10/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 12ms/step - loss: 5.6274 - val_loss: 6.2055\n",
      "Epoch 11/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 12ms/step - loss: 5.6202 - val_loss: 6.2115\n",
      "Epoch 12/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m602s\u001b[0m 12ms/step - loss: 5.6213 - val_loss: 6.2156\n",
      "Epoch 13/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 12ms/step - loss: 5.6191 - val_loss: 6.2212\n",
      "Epoch 14/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 12ms/step - loss: 5.6158 - val_loss: 6.2207\n",
      "Epoch 15/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 12ms/step - loss: 5.6142 - val_loss: 6.2257\n",
      "Epoch 16/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 12ms/step - loss: 5.6147 - val_loss: 6.2261\n",
      "Epoch 17/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 13ms/step - loss: 5.6155 - val_loss: 6.2259\n",
      "Epoch 18/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 12ms/step - loss: 5.6153 - val_loss: 6.2308\n",
      "Epoch 19/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 12ms/step - loss: 5.6191 - val_loss: 6.2327\n",
      "Epoch 20/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 13ms/step - loss: 5.6203 - val_loss: 6.2343\n",
      "Epoch 21/50\n",
      "\u001b[1m49092/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 12ms/step - loss: 5.6210 - val_loss: 6.2351\n",
      "Epoch 22/50\n",
      "\u001b[1m43032/49092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1:18\u001b[0m 13ms/step - loss: 5.6135"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = len(unique_words)\n",
    "embedding_dim = 100  # Dimension of the embedding vectors\n",
    "\n",
    "# Define the model\n",
    "input_layer = tf.keras.layers.Input(shape=(1,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)(input_layer)\n",
    "flatten_layer = tf.keras.layers.Flatten()(embedding_layer)\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')(flatten_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=40, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top related words to 'harry':\n",
      "hermione: 0.6103\n",
      "ron: 0.5970\n",
      "james: 0.5350\n",
      "dirk: 0.4983\n",
      "ringleaders: 0.4949\n",
      "feebly: 0.4848\n",
      "witheringly: 0.4692\n",
      "aberforth: 0.4628\n",
      "magorian: 0.4619\n",
      "mournfully: 0.4601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a model to extract embeddings\n",
    "embedding_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=1).output)\n",
    "embeddings = embedding_model.get_weights()[0]  # Get the weights of the embedding layer\n",
    "\n",
    "# Function to get embedding for a word\n",
    "def get_embedding(word):\n",
    "    word_idx = word_to_idx[word]\n",
    "    return embeddings[word_idx]\n",
    "\n",
    "# Compute similarity between the target word and all other words\n",
    "def get_most_similar_words(target_word, top_n=10):\n",
    "    target_embedding = get_embedding(target_word)\n",
    "    similarities = []\n",
    "    \n",
    "    for idx, word in idx_to_word.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_embedding = embeddings[idx]\n",
    "        similarity = cosine_similarity([target_embedding], [word_embedding])[0][0]\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "target_word = \"harry\"\n",
    "top_related_words = get_most_similar_words(target_word, top_n=10)\n",
    "\n",
    "print(f\"Top related words to '{target_word}':\")\n",
    "for word, similarity in top_related_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top related words to 'harry':\n",
      "hermione: 0.7459\n",
      "ron: 0.7029\n",
      "neville: 0.6917\n",
      "hagrid: 0.6775\n",
      "he: 0.6774\n",
      "snape: 0.6748\n",
      "fred: 0.6644\n",
      "wormtail: 0.6555\n",
      "dudley: 0.6297\n",
      "she: 0.6290\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a model to extract embeddings\n",
    "embedding_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=1).output)\n",
    "embeddings = embedding_model.get_weights()[0]  # Get the weights of the embedding layer\n",
    "\n",
    "# Function to get embedding for a word\n",
    "def get_embedding(word):\n",
    "    word_idx = word_to_idx[word]\n",
    "    return embeddings[word_idx]\n",
    "\n",
    "# Compute similarity between the target word and all other words\n",
    "def get_most_similar_words(target_word, top_n=10):\n",
    "    target_embedding = get_embedding(target_word)\n",
    "    similarities = []\n",
    "    \n",
    "    for idx, word in idx_to_word.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_embedding = embeddings[idx]\n",
    "        similarity = cosine_similarity([target_embedding], [word_embedding])[0][0]\n",
    "        similarities.append((word, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# Example usage\n",
    "target_word = \"harry\"\n",
    "top_related_words = get_most_similar_words(target_word, top_n=10)\n",
    "\n",
    "print(f\"Top related words to '{target_word}':\")\n",
    "for word, similarity in top_related_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
