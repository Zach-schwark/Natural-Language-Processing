{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Emotions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_train = load_dataset(\"csv\", data_files=\"./Swahili Emotion Data/emo_train.csv\", encoding = \"ISO-8859-1\")\n",
    "emo_valid = load_dataset(\"csv\", data_files=\"./Swahili Emotion Data/emo_valid.csv\", encoding = \"ISO-8859-1\")\n",
    "emo_test = load_dataset(\"csv\", data_files=\"./Swahili Emotion Data/emo_test.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \t0 - neutral\n",
    "#\t1 - joy (furaha)\n",
    "#\t2 - anger (hasira)\n",
    "#\t3 - sadness (huzuni)\n",
    "#\t4 - disgust (machukizo)\n",
    "#\t5 - suprise (mshangao)\n",
    "#\t6 - fear (woga)\n",
    "\n",
    "\n",
    "classes = ['neutral','joy','anger','sadness','disgust','suprise','fear']\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model\n",
    "### AfriBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zachs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\zachs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"castorini/afriberta_base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"castorini/afriberta_base\", num_labels=len(classes),id2label=id2class, label2id=class2id,problem_type = \"multi_label_classification\")\n",
    "\n",
    "def model_init(trial):\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        \"castorini/afriberta_base\",\n",
    "        num_labels=len(classes),\n",
    "        id2label=id2class,\n",
    "        label2id=class2id,\n",
    "        problem_type = \"multi_label_classification\"\n",
    "    )\n",
    "\n",
    "tokenizer.model_max_length = 512 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Emotion Dataset using the Trainer\n",
    "\n",
    "import ast\n",
    "\n",
    "def process_label(data):\n",
    "    lables = []\n",
    "    for element in data['labels']:\n",
    "        element = element.replace(\"[\",\"\")\n",
    "        element = element.replace(\"]\",\"\")\n",
    "        element = element.replace(\" \",\"\")\n",
    "        element_list = element.split(\",\")\n",
    "        label_list = [int(item) for item in element_list]\n",
    "        lables.append(label_list)\n",
    "    data['labels'] = lables\n",
    "    return data\n",
    "\n",
    "tokenised_trained_emotion = emo_train.map(process_label, batched=True)\n",
    "tokenised_valid_emotion = emo_valid.map(process_label, batched=True)\n",
    "tokenised_test_emotion = emo_test.map(process_label, batched=True)\n",
    "\n",
    "\n",
    "def tokenize_function(data):\n",
    "    all_labels = data['labels']\n",
    "    labels = [0. for i in range(len(classes))]\n",
    "    for label in all_labels:\n",
    "        label_id = label\n",
    "        labels[label_id] = 1.\n",
    "    \n",
    "    data = tokenizer(data[\"text\"], padding = \"max_length\", truncation=True)\n",
    "    data['labels'] = labels\n",
    "    return data\n",
    "\n",
    "\n",
    "tokenised_trained_emotion = tokenised_trained_emotion.map(tokenize_function)\n",
    "tokenised_valid_emotion = tokenised_valid_emotion.map(tokenize_function)\n",
    "tokenised_test_emotion = tokenised_test_emotion.map(tokenize_function)\n",
    "\n",
    "tokenised_trained_emotion = tokenised_trained_emotion.rename_column(\"labels\",\"label\")\n",
    "tokenised_valid_emotion = tokenised_valid_emotion.rename_column(\"labels\",\"label\")\n",
    "tokenised_test_emotion = tokenised_test_emotion.rename_column(\"labels\",\"label\")\n",
    "\n",
    "#print(tokenised_trained_emotion['train']['label'])\n",
    "\n",
    "    \n",
    "small_train_dataset = tokenised_trained_emotion[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenised_valid_emotion[\"train\"].shuffle(seed=42).select(range(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea264397d4244899ab5d7378cbf7904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3444, 'grad_norm': 3.328479766845703, 'learning_rate': 1.99571124883583e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2771, 'grad_norm': 2.6560094356536865, 'learning_rate': 1.679032218554042e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff70b85e847a4c4b8a0bddd20ccdd012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25942298769950867, 'eval_accuracy': 0.8972353783456328, 'eval_f1': 0.6084767100293748, 'eval_precision': 0.7382892057026477, 'eval_recall': 0.5174875089221984, 'eval_runtime': 20.7258, 'eval_samples_per_second': 62.579, 'eval_steps_per_second': 7.865, 'epoch': 1.0}\n",
      "{'loss': 0.247, 'grad_norm': 2.880528688430786, 'learning_rate': 1.3623531882722536e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2056, 'grad_norm': 2.581974506378174, 'learning_rate': 1.0456741579904651e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67990f4678df4d08b89752334804298b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2587554454803467, 'eval_accuracy': 0.8991078312589492, 'eval_f1': 0.645784996133024, 'eval_precision': 0.7046413502109705, 'eval_recall': 0.5960028551034975, 'eval_runtime': 25.4176, 'eval_samples_per_second': 51.028, 'eval_steps_per_second': 6.413, 'epoch': 2.0}\n",
      "{'loss': 0.1957, 'grad_norm': 2.3895490169525146, 'learning_rate': 7.289951277086768e-06, 'epoch': 2.05}\n",
      "{'loss': 0.1553, 'grad_norm': 1.16203773021698, 'learning_rate': 4.1231609742688845e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1522, 'grad_norm': 3.737990379333496, 'learning_rate': 9.563706714510009e-07, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec03d9a579f4a55aeca34cdf0cbaf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26952168345451355, 'eval_accuracy': 0.8970150897675956, 'eval_f1': 0.6462353386303443, 'eval_precision': 0.6876006441223832, 'eval_recall': 0.609564596716631, 'eval_runtime': 24.9564, 'eval_samples_per_second': 51.971, 'eval_steps_per_second': 6.531, 'epoch': 3.0}\n",
      "{'train_runtime': 4010.2616, 'train_samples_per_second': 7.28, 'train_steps_per_second': 0.91, 'train_loss': 0.22202677323177591, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3651, training_loss=0.22202677323177591, metrics={'train_runtime': 4010.2616, 'train_samples_per_second': 7.28, 'train_steps_per_second': 0.91, 'total_flos': 5139287096684544.0, 'train_loss': 0.22202677323177591, 'epoch': 3.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),\n",
    "    }\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "   def compute_loss(self, model, inputs, return_outputs=False):\n",
    "      outputs = model(\n",
    "          input_ids=inputs['input_ids'],\n",
    "          attention_mask=inputs['attention_mask'],\n",
    "      )\n",
    "      cls_logits = outputs.logits\n",
    "      loss = torch.nn.BCEWithLogitsLoss()(cls_logits.float(),\n",
    "                                       inputs['labels'].float())\n",
    "      return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels.astype(int).reshape(-1), predictions, average=\"binary\")\n",
    "    acc = accuracy_score(labels.astype(int).reshape(-1), predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"emotion_model\",\n",
    "   learning_rate = 2.3123902791176186e-05,\n",
    "   adam_epsilon = 1e-8, # default\n",
    "   eval_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_trained_emotion['train'],\n",
    "    eval_dataset=tokenised_valid_emotion['train'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #model_init=model_init,\n",
    "    callbacks=[EarlyStoppingCallback]\n",
    ")\n",
    "\n",
    "#best_trial = trainer.hyperparameter_search(\n",
    "#    direction=\"maximize\",\n",
    "#    backend=\"optuna\",\n",
    "#    hp_space=optuna_hp_space,\n",
    "#    n_trials=5,\n",
    "#)\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5629\n",
      "f1_score: 0.5947\n",
      "recall: 0.5484\n",
      "precision: 0.6773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in test_dataset:\n",
    "            inputs = {k: torch.tensor(v).unsqueeze(0).to(model.device) for k, v in example.items() \n",
    "                      if k in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "            label = example['label']\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_true_labels.append(label)\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='macro')\n",
    "    recall = recall_score(all_true_labels, all_predictions, average='macro')\n",
    "    precision = precision_score(all_true_labels, all_predictions, average='macro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_model(model, tokenised_test_emotion['train'])\n",
    "\n",
    "# Print results\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./first_finetuning_model\")\n",
    "model.save_pretrained(\"./first_finetuning_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
