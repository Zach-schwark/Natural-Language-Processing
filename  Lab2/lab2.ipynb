{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zachs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "nltk.download('punkt')\n",
    "\n",
    "y_book_array = [\"HP1\", \"HP2\", \"HP3\", \"HP4\"]\n",
    "\n",
    "\n",
    "# Function to read and preprocess text\n",
    "def read_and_preprocess(files):\n",
    "    corpus = []\n",
    "    book_tokens = []\n",
    "   \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read().lower()\n",
    "            # Split text into pages\n",
    "            pages = text.split('\\n')\n",
    "            \n",
    "            book_pages_tokens = []\n",
    "            for page in pages:\n",
    "                # Remove punctuation\n",
    "                page = re.sub(r'[^\\w\\s]', '', page)\n",
    "                # Tokenize\n",
    "                tokens = word_tokenize(page)\n",
    "                corpus.extend(tokens)\n",
    "                book_pages_tokens.append(tokens)\n",
    "            \n",
    "            book_tokens.append(book_pages_tokens)\n",
    "    return corpus, book_tokens\n",
    "\n",
    "# Specify the files for the seven books\n",
    "files = ['HarryPotter/HP1.txt', 'HarryPotter/HP2.txt', 'HarryPotter/HP3.txt', 'HarryPotter/HP4.txt']\n",
    "\n",
    "# Preprocess the text\n",
    "# tokens is now a 3d array with each sub array has sub-subarrays that are each page for each book\n",
    "corpus, tokens = read_and_preprocess(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigrams</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mr and mrs, and mrs dursley, mrs dursley of, ...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[met for several, for several years, several y...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the cat it, cat it stared, it stared back, st...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[calls and shouted, and shouted a, shouted a b...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[he found it, found it a, it a lot, a lot hard...</td>\n",
       "      <td>HP1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>[harry potter and, potter and the, and the gob...</td>\n",
       "      <td>HP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>[harry potter and, potter and the, and the gob...</td>\n",
       "      <td>HP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>[harry potter and, potter and the, and the gob...</td>\n",
       "      <td>HP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>[harry thanks george, thanks george muttered, ...</td>\n",
       "      <td>HP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>[]</td>\n",
       "      <td>HP4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2023 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               trigrams book\n",
       "0     [mr and mrs, and mrs dursley, mrs dursley of, ...  HP1\n",
       "1     [met for several, for several years, several y...  HP1\n",
       "2     [the cat it, cat it stared, it stared back, st...  HP1\n",
       "3     [calls and shouted, and shouted a, shouted a b...  HP1\n",
       "4     [he found it, found it a, it a lot, a lot hard...  HP1\n",
       "...                                                 ...  ...\n",
       "2018  [harry potter and, potter and the, and the gob...  HP4\n",
       "2019  [harry potter and, potter and the, and the gob...  HP4\n",
       "2020  [harry potter and, potter and the, and the gob...  HP4\n",
       "2021  [harry thanks george, thanks george muttered, ...  HP4\n",
       "2022                                                 []  HP4\n",
       "\n",
       "[2023 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_ngrams_from_tokens(tokens, book,  n):\n",
    "    # Step 1: Initialize an empty list to store N-grams\n",
    "    n_grams = []\n",
    "    y_array = []\n",
    "    # Step 2: Generate N-grams\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        n_gram = ' '.join(tokens[i:i+n])\n",
    "        n_grams.append(n_gram)\n",
    "        y_array.append(book)\n",
    "    return n_grams, y_array\n",
    "\n",
    "for i in range(len(files)):\n",
    "    if i ==0:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        # for each page \n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 3)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        x_y_trigrams = pd.DataFrame({'trigrams': pages_trigramed, 'book': pages_y_array})\n",
    "    else:\n",
    "        pages_trigramed = []\n",
    "        pages_y_array = []\n",
    "        for j in range(len(tokens[i])): \n",
    "            trigrams, trigrams_y_array = extract_ngrams_from_tokens(tokens[i][j],y_book_array[i], 3)\n",
    "            pages_trigramed.append(trigrams)\n",
    "            pages_y_array.append(y_book_array[i])\n",
    "        book_x_y_trigrams = pd.DataFrame({\"trigrams\": pages_trigramed, \"book\": pages_y_array})\n",
    "        x_y_trigrams = pd.concat([x_y_trigrams, book_x_y_trigrams], ignore_index=True)\n",
    "    \n",
    "display(x_y_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(x_y_trigrams[\"trigrams\"], x_y_trigrams[\"book\"], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive Bayes Classifier </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Calculating the priors\n",
    "def get_priors(y_train):\n",
    "    book_counts = Counter(y_train)\n",
    "    total_count = len(y_train)\n",
    "    priors = {}\n",
    "    for i in book_counts:\n",
    "        priors.update({i: book_counts[i]/total_count})\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the occurences\n",
    "\n",
    "def get_occurences(n_gram, x_train, y_train, classes):\n",
    "    occurences = {i: 0 for i in classes}\n",
    "    \n",
    "    # \"trigrams\" is the trigrams for a page\n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        if n_gram in trigrams:\n",
    "            occurences[book] += 1\n",
    "        return occurences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caluclating the likelihood\n",
    "#vocab size = len(page)\n",
    "def get_likelihood(book_index,n_gram, n_gram_mapping, total_trigrams_per_class, vocab_size):\n",
    "    delta = 0.1\n",
    "    \n",
    "    if n_gram in n_gram_mapping:\n",
    "        numerator = delta + n_gram_mapping[n_gram][book_index]\n",
    "        denominator = delta * vocab_size + total_trigrams_per_class[book_index]\n",
    "    else:\n",
    "        numerator = delta\n",
    "        denominator = delta * vocab_size + total_trigrams_per_class[book_index]\n",
    "    \n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram mapping -> mapping from each n_gram to its occurences count in each book and \n",
    "# calculates the total number of trigrams per class\n",
    "\n",
    "def n_gram_mapping(x_train, y_train, classes):\n",
    "    n_gram_mapping = {}\n",
    "    total_trigrams_per_class = {c: 0 for c in classes}\n",
    "    \n",
    "    # \"trigrams\" is the trigrams for a page\n",
    "    for trigrams, book in zip(x_train, y_train):\n",
    "        # \"n_gram\" is each trigram in the page of trigrams\n",
    "        for n_gram in trigrams:\n",
    "            if n_gram not in n_gram_mapping:\n",
    "                n_gram_mapping[n_gram] = get_occurences(n_gram, x_train, y_train, classes)\n",
    "            total_trigrams_per_class[book] += 1\n",
    "    return n_gram_mapping, total_trigrams_per_class\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the probability of page being in a book\n",
    "\n",
    "def prob_page_in_book(page, priors, n_gram_mapping, total_trigrams_per_class, classes, vocab_size):\n",
    "    probs = []\n",
    "    \n",
    "    for c in classes:\n",
    "        prior = priors[c]\n",
    "        likelihood = 1\n",
    "        \n",
    "        for n_gram in page:\n",
    "            likelihood *= get_likelihood(c, n_gram, n_gram_mapping, total_trigrams_per_class, vocab_size)\n",
    "        probs.append(prior * likelihood)\n",
    "    return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding \n",
    "# used to identify the class with the highest probability\n",
    "\n",
    "def convert_prob_to_one_hot(probs):\n",
    "    probs = np.array(probs)\n",
    "    one_hots = np.zeros(len(probs))\n",
    "    one_hot_index = np.random.choice(np.where(probs == probs.max())[0])\n",
    "    one_hots[one_hot_index] = 1\n",
    "    return one_hots\n",
    "\n",
    "#getting the actual one hot \n",
    "\n",
    "def get_actual_one_hot_encoding(index, length):\n",
    "    one_hots = np.zeros(length)\n",
    "    one_hots[index] = 1\n",
    "    return one_hots\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "#def filter_books(x, y, train_ratio = 0.6):\n",
    "#    x_train, x_test = [], []\n",
    "#    y_train, y_test = [], []\n",
    "#    \n",
    "#    for i in range(len(x)):\n",
    "#        split_idx = int(len(x[i]) * train_ratio)\n",
    "#        X_train.append(x[i][:split_idx])\n",
    "#        y_train.append(y[i][:split_idx])\n",
    "#        X_test.append(x[i][split_idx:])\n",
    "#        y_test.append(y[i][split_idx:])\n",
    "#        \n",
    "#    X_train_flat = [item for sublist in x_train for item in sublist]\n",
    "#    y_train_flat = [item for sublist in y_train for item in sublist]\n",
    "#    X_test_flat = [item for sublist in x_test for item in sublist]\n",
    "#    y_test_flat = [item for sublist in y_test for item in sublist]\n",
    "#    \n",
    "#    return X_train_flat, X_test_flat, y_train_flat, y_test_flat\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [1. 0. 0. 0.]\n",
      "Actual: [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# X_train_val, X_test, y_train_val, y_test\n",
    "# X_train, X_val, y_train, y_val \n",
    "\n",
    "\n",
    "# Split the data\n",
    "\n",
    "# Get priors and n-gram mappings\n",
    "#classes = list(set(y_train))\n",
    "classes = y_book_array\n",
    "priors = get_priors(y_train)\n",
    "ngram_mapping, total_trigrams_per_class = n_gram_mapping(X_train, y_train, classes)\n",
    "ngram_set = set()\n",
    "for trigrams in X_train:\n",
    "    for ngram in trigrams:\n",
    "        ngram_set.add(ngram)\n",
    "vocab_size = len(ngram_set)\n",
    "\n",
    "# Predict for a test page\n",
    "page = X_test.iloc[0] # page will be of type list\n",
    "probs = prob_page_in_book(page, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size)\n",
    "one_hot_prediction = convert_prob_to_one_hot(probs)\n",
    "\n",
    "# Compare with the actual label\n",
    "actual_one_hot = get_actual_one_hot_encoding(classes.index(y_test.iloc[0]), len(classes))\n",
    "print(\"Predicted:\", one_hot_prediction)\n",
    "print(\"Actual:\", actual_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_page_class(page, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size):\n",
    "    probs = prob_page_in_book(page, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size)\n",
    "    return np.argmax(probs)  #return the index of the highest probability\n",
    "\n",
    "def calculate_accuracy(X_test, y_test, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(X_test)\n",
    "\n",
    "    for i, page in enumerate(X_test):\n",
    "        index_of_predicted_class = predict_page_class(page, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size)\n",
    "        predicted_class = classes[index_of_predicted_class]\n",
    "        actual_class = y_test.iloc[i]\n",
    "        if predicted_class == actual_class:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 17.53%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the test set\n",
    "accuracy = calculate_accuracy(X_test, y_test, priors, ngram_mapping, total_trigrams_per_class, classes, vocab_size)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
